{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-15T17:52:30.036527Z","iopub.execute_input":"2023-08-15T17:52:30.037273Z","iopub.status.idle":"2023-08-15T17:52:30.052767Z","shell.execute_reply.started":"2023-08-15T17:52:30.037231Z","shell.execute_reply":"2023-08-15T17:52:30.051868Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/input/fashion-mnist/fashion-mnist_test.csv\n/kaggle/input/fashion-mnist/fashion-mnist_train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.utils.np_utils import to_categorical\n\ntrain_df = pd.read_csv('/kaggle/input/fashion-mnist/fashion-mnist_train.csv')\n#train_df.head()\ny = train_df['label']\nx = train_df.drop(['label'], axis=1, inplace=False)\nx = np.array(x)\nx = (x/255).astype('float32')\ny = to_categorical(y)\nx.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-15T17:52:33.228232Z","iopub.execute_input":"2023-08-15T17:52:33.228647Z","iopub.status.idle":"2023-08-15T17:52:39.180688Z","shell.execute_reply.started":"2023-08-15T17:52:33.228615Z","shell.execute_reply":"2023-08-15T17:52:39.179449Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(60000, 784)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport time\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)\ny_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-15T17:53:02.427999Z","iopub.execute_input":"2023-08-15T17:53:02.428425Z","iopub.status.idle":"2023-08-15T17:53:03.500309Z","shell.execute_reply.started":"2023-08-15T17:53:02.428390Z","shell.execute_reply":"2023-08-15T17:53:03.499175Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(51000, 10)"},"metadata":{}}]},{"cell_type":"code","source":"def sigmoid(s):\n    return 1/(1 + np.exp(-s))\n\ndef sigmoid_derv(s):\n    return s * (1 - s)\n\ndef softmax(s):\n    exps = np.exp(s - np.max(s, axis=1, keepdims=True))\n    return exps/np.sum(exps, axis=1, keepdims=True)\n\ndef error(pred, real):\n    n_samples = real.shape[0]\n    res = pred - real\n    return res/n_samples\n\ndef cross_entropy(pred, real):\n    n_samples = real.shape[0]\n    logp = - np.log(pred[np.arange(n_samples), real.argmax(axis=1)])\n    loss = np.sum(logp)/n_samples\n    return loss\n\nclass MyNN:\n    def __init__(self, x, y):\n        self.x = x\n        neurons = 256\n        self.lr = 0.05\n        ip_dim = x.shape[1]\n        op_dim = y.shape[1]\n\n        self.w1 = np.random.randn(ip_dim, neurons)\n        self.b1 = np.zeros((1, neurons))\n        self.w2 = np.random.randn(neurons, neurons)\n        self.b2 = np.zeros((1, neurons))\n        self.w3 = np.random.randn(neurons, op_dim)\n        self.b3 = np.zeros((1, op_dim))\n        self.y = y\n\n    def feedforward(self):\n        z1 = np.dot(self.x, self.w1) + self.b1\n        self.a1 = sigmoid(z1)\n        z2 = np.dot(self.a1, self.w2) + self.b2\n        self.a2 = sigmoid(z2)\n        z3 = np.dot(self.a2, self.w3) + self.b3\n        self.a3 = softmax(z3)\n        \n    def backprop(self):\n        loss = cross_entropy(self.a3, self.y)\n        print('Error :', loss)\n        \n        a3_delta = error(self.a3, self.y) # w3\n        z2_delta = np.dot(a3_delta, self.w3.T)\n        a2_delta = z2_delta * sigmoid_derv(self.a2) # w2\n        z1_delta = np.dot(a2_delta, self.w2.T)\n        a1_delta = z1_delta * sigmoid_derv(self.a1) # w1\n\n        self.w3 -= self.lr * np.dot(self.a2.T, a3_delta)\n        self.b3 -= self.lr * np.sum(a3_delta, axis=0, keepdims=True)\n        self.w2 -= self.lr * np.dot(self.a1.T, a2_delta)\n        self.b2 -= self.lr * np.sum(a2_delta, axis=0)\n        self.w1 -= self.lr * np.dot(self.x.T, a1_delta)\n        self.b1 -= self.lr * np.sum(a1_delta, axis=0)\n\n    def predict(self, data):\n        self.x = data\n        self.feedforward()\n        return self.a3.argmax()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-15T17:53:06.982447Z","iopub.execute_input":"2023-08-15T17:53:06.982851Z","iopub.status.idle":"2023-08-15T17:53:07.003063Z","shell.execute_reply.started":"2023-08-15T17:53:06.982818Z","shell.execute_reply":"2023-08-15T17:53:07.001914Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = MyNN(x_train, np.array(y_train))\n\nepochs = 500\nfor x in range(epochs):\n    print('Epoch {}'.format(x+1))\n    model.feedforward()\n    model.backprop()\n\ndef get_acc(x, y):\n    acc = 0\n    for xx,yy in zip(x, y):\n        s = model.predict(xx)\n        if s == np.argmax(yy):\n            acc +=1\n    return acc/len(x)*100\n\nprint(\"Training accuracy : \", get_acc(x_train, np.array(y_train)))\nprint(\"Test accuracy : \", get_acc(x_val, np.array(y_val)))","metadata":{"execution":{"iopub.status.busy":"2023-08-15T17:54:36.273496Z","iopub.execute_input":"2023-08-15T17:54:36.273925Z","iopub.status.idle":"2023-08-15T18:06:04.108131Z","shell.execute_reply.started":"2023-08-15T17:54:36.273893Z","shell.execute_reply":"2023-08-15T18:06:04.106365Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1\nError : 22.04970395840946\nEpoch 2\nError : 15.584357483481282\nEpoch 3\nError : 11.415734727106209\nEpoch 4\nError : 9.6179077728244\nEpoch 5\nError : 8.483192511894048\nEpoch 6\nError : 7.678518823553695\nEpoch 7\nError : 7.086940260989644\nEpoch 8\nError : 6.637493413015751\nEpoch 9\nError : 6.289816061817968\nEpoch 10\nError : 6.0150294008632175\nEpoch 11\nError : 5.789751523020377\nEpoch 12\nError : 5.5964809018265065\nEpoch 13\nError : 5.423786121722877\nEpoch 14\nError : 5.265043418443148\nEpoch 15\nError : 5.11673170467995\nEpoch 16\nError : 4.977055198016352\nEpoch 17\nError : 4.845076311555279\nEpoch 18\nError : 4.720247863600963\nEpoch 19\nError : 4.602185454533338\nEpoch 20\nError : 4.490563850473279\nEpoch 21\nError : 4.385071376442712\nEpoch 22\nError : 4.285391648484292\nEpoch 23\nError : 4.191199991377771\nEpoch 24\nError : 4.102167346980874\nEpoch 25\nError : 4.017966566034534\nEpoch 26\nError : 3.9382787544604696\nEpoch 27\nError : 3.862798853178782\nEpoch 28\nError : 3.7912396339486745\nEpoch 29\nError : 3.723333572518525\nEpoch 30\nError : 3.6588328772718532\nEpoch 31\nError : 3.5975084449853116\nEpoch 32\nError : 3.539148429606782\nEpoch 33\nError : 3.48355680785145\nEpoch 34\nError : 3.4305520877196876\nEpoch 35\nError : 3.3799661825785883\nEpoch 36\nError : 3.3316434279091833\nEpoch 37\nError : 3.2854397066290484\nEpoch 38\nError : 3.2412216462766907\nEpoch 39\nError : 3.1988658543215513\nEpoch 40\nError : 3.158258170899434\nEpoch 41\nError : 3.1192929378196204\nEpoch 42\nError : 3.0818722978481627\nEpoch 43\nError : 3.0459055411319986\nEpoch 44\nError : 3.0113085073563237\nEpoch 45\nError : 2.978003041025626\nEpoch 46\nError : 2.9459164917529397\nEpoch 47\nError : 2.9149812540389237\nEpoch 48\nError : 2.8851343466880075\nEpoch 49\nError : 2.856317034113675\nEpoch 50\nError : 2.8284744887263704\nEpoch 51\nError : 2.8015554890431624\nEpoch 52\nError : 2.775512146059512\nEpoch 53\nError : 2.7502996515047147\nEpoch 54\nError : 2.725876044180724\nEpoch 55\nError : 2.7022019929376455\nEpoch 56\nError : 2.679240596267989\nEpoch 57\nError : 2.656957199031735\nEpoch 58\nError : 2.6353192266844476\nEpoch 59\nError : 2.6142960367940904\nEpoch 60\nError : 2.593858786828636\nEpoch 61\nError : 2.573980316427843\nEpoch 62\nError : 2.554635041860504\nEpoch 63\nError : 2.535798860229435\nEpoch 64\nError : 2.5174490612012685\nEpoch 65\nError : 2.4995642444862924\nEpoch 66\nError : 2.482124241821937\nEpoch 67\nError : 2.4651100426972405\nEpoch 68\nError : 2.448503723426375\nEpoch 69\nError : 2.432288379422481\nEpoch 70\nError : 2.4164480606576495\nEpoch 71\nError : 2.4009677103522638\nEpoch 72\nError : 2.3858331069467638\nEpoch 73\nError : 2.371030809392572\nEpoch 74\nError : 2.3565481057699924\nEpoch 75\nError : 2.342372965206662\nEpoch 76\nError : 2.328493993034926\nEpoch 77\nError : 2.31490038909301\nEpoch 78\nError : 2.301581909044844\nEpoch 79\nError : 2.2885288285685492\nEpoch 80\nError : 2.2757319102446028\nEpoch 81\nError : 2.263182372962129\nEpoch 82\nError : 2.250871863655472\nEpoch 83\nError : 2.238792431182602\nEpoch 84\nError : 2.2269365021613363\nEpoch 85\nError : 2.2152968585876023\nEpoch 86\nError : 2.203866617071116\nEpoch 87\nError : 2.192639209536657\nEpoch 88\nError : 2.1816083652528264\nEpoch 89\nError : 2.1707680940638885\nEpoch 90\nError : 2.160112670713755\nEpoch 91\nError : 2.149636620163921\nEpoch 92\nError : 2.1393347038192614\nEpoch 93\nError : 2.1292019065868866\nEpoch 94\nError : 2.1192334247037774\nEpoch 95\nError : 2.1094246542784494\nEpoch 96\nError : 2.0997711805004156\nEpoch 97\nError : 2.090268767478402\nEpoch 98\nError : 2.0809133486740863\nEpoch 99\nError : 2.0717010179023894\nEpoch 100\nError : 2.062628020872247\nEpoch 101\nError : 2.053690747243359\nEpoch 102\nError : 2.0448857231750646\nEpoch 103\nError : 2.036209604343464\nEpoch 104\nError : 2.0276591694026402\nEpoch 105\nError : 2.0192313138655007\nEpoch 106\nError : 2.0109230443796613\nEpoch 107\nError : 2.002731473373982\nEpoch 108\nError : 1.9946538140518362\nEpoch 109\nError : 1.9866873757079462\nEpoch 110\nError : 1.978829559346542\nEpoch 111\nError : 1.9710778535795563\nEpoch 112\nError : 1.9634298307845712\nEpoch 113\nError : 1.9558831435031045\nEpoch 114\nError : 1.9484355210606699\nEpoch 115\nError : 1.9410847663907458\nEpoch 116\nError : 1.9338287530454945\nEpoch 117\nError : 1.9266654223767516\nEpoch 118\nError : 1.9195927808715483\nEpoch 119\nError : 1.9126088976272866\nEpoch 120\nError : 1.9057119019526927\nEpoch 121\nError : 1.89889998108182\nEpoch 122\nError : 1.8921713779896836\nEpoch 123\nError : 1.8855243892995621\nEpoch 124\nError : 1.878957363273463\nEpoch 125\nError : 1.8724686978787934\nEpoch 126\nError : 1.8660568389257124\nEpoch 127\nError : 1.8597202782709754\nEpoch 128\nError : 1.8534575520852614\nEpoch 129\nError : 1.847267239181909\nEpoch 130\nError : 1.8411479594057443\nEpoch 131\nError : 1.835098372081178\nEpoch 132\nError : 1.829117174519036\nEpoch 133\nError : 1.8232031005817049\nEpoch 134\nError : 1.8173549193061007\nEpoch 135\nError : 1.8115714335838102\nEpoch 136\nError : 1.8058514788974944\nEpoch 137\nError : 1.8001939221123484\nEpoch 138\nError : 1.7945976603211178\nEpoch 139\nError : 1.7890616197408715\nEpoch 140\nError : 1.7835847546595078\nEpoch 141\nError : 1.7781660464297542\nEpoch 142\nError : 1.7728045025083254\nEpoch 143\nError : 1.7674991555378046\nEpoch 144\nError : 1.7622490624688545\nEpoch 145\nError : 1.7570533037203797\nEpoch 146\nError : 1.7519109823753884\nEpoch 147\nError : 1.7468212234104297\nEpoch 148\nError : 1.7417831729566373\nEpoch 149\nError : 1.7367959975905898\nEpoch 150\nError : 1.7318588836533677\nEpoch 151\nError : 1.7269710365963529\nEpoch 152\nError : 1.7221316803524824\nEpoch 153\nError : 1.7173400567317925\nEpoch 154\nError : 1.7125954248402362\nEpoch 155\nError : 1.7078970605208417\nEpoch 156\nError : 1.7032442558163774\nEpoch 157\nError : 1.6986363184527566\nEpoch 158\nError : 1.6940725713424842\nEpoch 159\nError : 1.6895523521074785\nEpoch 160\nError : 1.6850750126206437\nEpoch 161\nError : 1.6806399185656042\nEpoch 162\nError : 1.6762464490140099\nEpoch 163\nError : 1.6718939960198482\nEpoch 164\nError : 1.667581964230196\nEpoch 165\nError : 1.6633097705118354\nEpoch 166\nError : 1.6590768435931502\nEpoch 167\nError : 1.6548826237207064\nEpoch 168\nError : 1.6507265623298883\nEpoch 169\nError : 1.6466081217289557\nEpoch 170\nError : 1.642526774795834\nEpoch 171\nError : 1.6384820046869693\nEpoch 172\nError : 1.634473304557509\nEpoch 173\nError : 1.630500177292097\nEpoch 174\nError : 1.6265621352455415\nEpoch 175\nError : 1.622658699992619\nEpoch 176\nError : 1.6187894020862976\nEpoch 177\nError : 1.6149537808236716\nEpoch 178\nError : 1.6111513840189413\nEpoch 179\nError : 1.6073817677828133\nEpoch 180\nError : 1.6036444963077368\nEpoch 181\nError : 1.5999391416584778\nEpoch 182\nError : 1.5962652835675581\nEpoch 183\nError : 1.5926225092352069\nEpoch 184\nError : 1.5890104131335\nEpoch 185\nError : 1.5854285968144815\nEpoch 186\nError : 1.5818766687221038\nEpoch 187\nError : 1.578354244007923\nEpoch 188\nError : 1.5748609443505481\nEpoch 189\nError : 1.5713963977789107\nEpoch 190\nError : 1.567960238499482\nEpoch 191\nError : 1.5645521067276322\nEpoch 192\nError : 1.561171648523352\nEpoch 193\nError : 1.5578185156316113\nEpoch 194\nError : 1.5544923653276463\nEpoch 195\nError : 1.5511928602674843\nEpoch 196\nError : 1.5479196683440173\nEpoch 197\nError : 1.5446724625489276\nEpoch 198\nError : 1.5414509208407463\nEpoch 199\nError : 1.5382547260192985\nEpoch 200\nError : 1.5350835656067314\nEpoch 201\nError : 1.5319371317352988\nEpoch 202\nError : 1.5288151210419783\nEpoch 203\nError : 1.5257172345699663\nEpoch 204\nError : 1.5226431776770077\nEpoch 205\nError : 1.519592659950454\nEpoch 206\nError : 1.516565395128871\nEpoch 207\nError : 1.5135611010299668\nEpoch 208\nError : 1.510579499484521\nEpoch 209\nError : 1.5076203162759718\nEpoch 210\nError : 1.504683281085266\nEpoch 211\nError : 1.5017681274405188\nEpoch 212\nError : 1.4988745926710438\nEpoch 213\nError : 1.496002417865261\nEpoch 214\nError : 1.4931513478320078\nEpoch 215\nError : 1.4903211310647722\nEpoch 216\nError : 1.4875115197083812\nEpoch 217\nError : 1.484722269527697\nEpoch 218\nError : 1.4819531398778918\nEpoch 219\nError : 1.479203893675919\nEpoch 220\nError : 1.4764742973728051\nEpoch 221\nError : 1.4737641209264467\nEpoch 222\nError : 1.4710731377746178\nEpoch 223\nError : 1.4684011248079358\nEpoch 224\nError : 1.4657478623425615\nEpoch 225\nError : 1.463113134092453\nEpoch 226\nError : 1.4604967271410099\nEpoch 227\nError : 1.4578984319119903\nEpoch 228\nError : 1.4553180421395906\nEpoch 229\nError : 1.452755354837613\nEpoch 230\nError : 1.4502101702676642\nEpoch 231\nError : 1.447682291906339\nEpoch 232\nError : 1.4451715264113671\nEpoch 233\nError : 1.4426776835867074\nEpoch 234\nError : 1.4402005763465884\nEpoch 235\nError : 1.4377400206784972\nEpoch 236\nError : 1.4352958356051406\nEpoch 237\nError : 1.4328678431453865\nEpoch 238\nError : 1.4304558682742243\nEpoch 239\nError : 1.4280597388817686\nEpoch 240\nError : 1.4256792857313436\nEpoch 241\nError : 1.4233143424166974\nEpoch 242\nError : 1.4209647453183816\nEpoch 243\nError : 1.4186303335593529\nEpoch 244\nError : 1.416310948959852\nEpoch 245\nError : 1.4140064359916165\nEpoch 246\nError : 1.4117166417314908\nEpoch 247\nError : 1.4094414158145103\nEpoch 248\nError : 1.4071806103865152\nEpoch 249\nError : 1.4049340800563916\nEpoch 250\nError : 1.4027016818480054\nEpoch 251\nError : 1.4004832751519167\nEpoch 252\nError : 1.398278721676962\nEpoch 253\nError : 1.3960878854017964\nEpoch 254\nError : 1.3939106325264725\nEpoch 255\nError : 1.39174683142416\nEpoch 256\nError : 1.389596352593084\nEpoch 257\nError : 1.3874590686087678\nEpoch 258\nError : 1.3853348540766734\nEpoch 259\nError : 1.3832235855852983\nEpoch 260\nError : 1.381125141659822\nEpoch 261\nError : 1.3790394027163557\nEpoch 262\nError : 1.3769662510168568\nEpoch 263\nError : 1.3749055706247626\nEpoch 264\nError : 1.372857247361383\nEpoch 265\nError : 1.370821168763094\nEpoch 266\nError : 1.3687972240393458\nEpoch 267\nError : 1.3667853040315205\nEpoch 268\nError : 1.3647853011726394\nEpoch 269\nError : 1.362797109447922\nEpoch 270\nError : 1.360820624356209\nEpoch 271\nError : 1.3588557428722219\nEpoch 272\nError : 1.3569023634096582\nEpoch 273\nError : 1.3549603857851065\nEpoch 274\nError : 1.3530297111827545\nEpoch 275\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(x\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeedforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m.\u001b[39mbackprop()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_acc\u001b[39m(x, y):\n","Cell \u001b[0;32mIn[9], line 39\u001b[0m, in \u001b[0;36mMyNN.feedforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeedforward\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 39\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb1\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma1 \u001b[38;5;241m=\u001b[39m sigmoid(z1)\n\u001b[1;32m     41\u001b[0m     z2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw2) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb2\n","File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/fashion-mnist/fashion-mnist_test.csv')\n# test_df.head()\ntest_df = test_df.drop(['label'], axis=1, inplace=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = np.array(test_df)/255.0\ntest_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nfor img in test_df:\n    preds.append(model.predict(img))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.imshow(test_df[7].reshape(28, 28), cmap='binary')","metadata":{},"execution_count":null,"outputs":[]}]}